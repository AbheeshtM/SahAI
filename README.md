# SahAI

**SahAI** is a voice-first, multisensory AI companion designed to assist visually impaired users by helping them understand and navigate their surroundings using natural, human-like audio descriptions.

This project is submitted under the **Student Track â€“ AI for Communities, Access & Public Impact**, with a focus on **accessibility, inclusion, and real-world societal impact**.

---

## ğŸŒ Problem Statement

Visually impaired individuals often lack accessible tools to understand their immediate environment.  
Most existing AI vision systems provide robotic object labels or rely on visual interfaces, which do not align with how blind users perceive and interact with the world.

---

## ğŸ’¡ Solution

SahAI combines **computer vision**, **conversational AI**, and **sound-based interaction** to deliver meaningful, voice-only descriptions of objects, people, and environments.

Users can interact with SahAI conversationally by asking questions such as:
- *â€œWhat is on my right?â€*
- *â€œIs someone in front of me?â€*
- *â€œWhat am I holding?â€*

SahAI also provides **assistive, voice-based navigation guidance** using publicly available map data.

---

## ğŸ”‘ Key Features

- Fully **voice-first interaction**
- Human-like, sensory descriptions using body-relative comparisons
- Conversational questionâ€“answer capability
- Assistive navigation using free map APIs
- Designed for **low-bandwidth** and **lightweight models**
- Privacy-first approach (no permanent image storage)

---

## ğŸ§  How It Works (High Level)

1. **Computer Vision** detects objects and spatial context
2. **AI Language Layer** converts detections into natural descriptions
3. **Voice Interface** delivers audio output for accessibility
4. **Map-Based Guidance** provides assistive navigation directions

---

## â™¿ Accessibility & Inclusion

SahAI is built specifically for visually impaired communities, prioritizing:
- No visual UI dependency
- Natural language explanations
- Inclusive and empathetic design

---

## âš ï¸ Scope & Limitations

- SahAI is an **assistive prototype**, not a medical or safety-critical system
- Navigation guidance is advisory and context-based
- Real-time accuracy depends on environmental conditions

---

## ğŸ“ Repository Contents

- `requirements.md` â€“ Functional and non-functional requirements (generated via Kiro)
- `design.md` â€“ System architecture and design overview (generated via Kiro)

---

## ğŸ Conclusion

SahAI aims to bridge the gap between vision and understanding by enabling visually impaired users to experience their surroundings through sound, awareness, and intelligent assistance.

